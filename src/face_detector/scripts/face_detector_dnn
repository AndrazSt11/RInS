#!/usr/bin/python3

import sys
import math
import os
import rospy
import dlib
import cv2
import numpy as np

import tf2_geometry_msgs
import tf2_ros

from sensor_msgs.msg import Image
from geometry_msgs.msg import PointStamped, Vector3, Pose
from cv_bridge import CvBridge, CvBridgeError

import message_filters
from face_detector.msg import FaceDetected

class face_detector_dnn:
    def __init__(self):
        rospy.init_node('face_detector', anonymous=True)

        # An object we use for converting images between ROS format and OpenCV format
        self.bridge = CvBridge()

        # Object we use for transforming between coordinate frames
        self.tf_buf = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buf)

        # The function for performin HOG face detection
        currentPath = os.path.dirname(os.path.abspath(__file__));
        self.face_net = cv2.dnn.readNetFromCaffe(currentPath + '/deploy.prototxt.txt',
                                                 currentPath + '/res10_300x300_ssd_iter_140000.caffemodel')

        # A help variable for holding the dimensions of the image
        self.dims = (0, 0, 0)
        self.mask_variance_treshold = 100

        # Detection publisher
        self.detection_publisher = rospy.Publisher('face_detection', FaceDetected);

        # Image subscribers
        self.rgb_subscriber = message_filters.Subscriber("/camera/rgb/image_raw", Image)
        self.depth_subscriber = message_filters.Subscriber("/camera/depth/image_raw", Image)

        # Sync rbg and depth message
        self.time_sync = message_filters.TimeSynchronizer([self.rgb_subscriber, self.depth_subscriber], 10)
        self.time_sync.registerCallback(self.register_images)

        self.rgb_message = None
        self.depth_message = None


    def register_images(self, rgb_message, depth_message):   
        self.rgb_message = rgb_message
        self.depth_message = depth_message

    
    def find_mask(self, face_region):
        h = face_region.shape[0]
        w = face_region.shape[1]

        # Extract mask region
        # TODO: test on given images!
        y1 = int(h * 0.75)
        y2 = int(h * 0.85)
        x1 = int(w * 0.35)
        x2 = int(w * 0.65)
        mask_region = face_region[y1:y2, x1:x2]

        # cv2.imshow("Image window", mask_region)
        # cv2.waitKey(1)

        # Compute variance
        n = mask_region.shape[0] * mask_region.shape[1]
        avg = np.sum(mask_region, axis=(0,1)) / n

        variance = [0, 0, 0]
        for y in range(0, mask_region.shape[0]):
            for x in range(0, mask_region.shape[1]):
                variance += np.power(mask_region[y, x] - avg, 2)
        variance /= n
        variance = np.sum(variance)
        print("Variance:", variance)

        if variance < self.mask_variance_treshold:
            return True

        return False


    def get_pose(self, coords, dist, stamp):
        # Calculate the position of the detected face
        k_f = 554 # kinect focal length in pixels

        x1, x2, y1, y2 = coords

        face_x = self.dims[1] / 2 - (x1+x2)/2.
        face_y = self.dims[0] / 2 - (y1+y2)/2.

        angle_to_target = np.arctan2(face_x,k_f)

        # Get the angles in the base_link relative coordinate system
        x, y = dist*np.cos(angle_to_target), dist*np.sin(angle_to_target)

        # Define a stamped message for transformation - in the "camera rgb frame"
        point_s = PointStamped()
        point_s.point.x = -y
        point_s.point.y = 0
        point_s.point.z = x
        point_s.header.frame_id = "camera_rgb_optical_frame"
        point_s.header.stamp = stamp

        # Get the point in the "map" coordinate system
        try:
            point_world = self.tf_buf.transform(point_s, "map")

            faceDetectedMsg = FaceDetected()
            faceDetectedMsg.header.frame_id = "map"
            faceDetectedMsg.header.stamp = stamp
            faceDetectedMsg.world_x = point_world.point.x
            faceDetectedMsg.world_y = point_world.point.y
            faceDetectedMsg.world_z = point_world.point.z

        except Exception as e:
            faceDetectedMsg = FaceDetected()
            print(e)

        return faceDetectedMsg
    

    def find_faces(self):
        rospy.loginfo('New image processing started')

        if((self.rgb_message is not None) or (self.depth_message is not None)):
            # Convert the images into a OpenCV (numpy) format
            try:
                # rgb_image = self.bridge.imgmsg_to_cv2(rgb_image_message, "bgr8")
                rgb_image = self.bridge.imgmsg_to_cv2(self.rgb_message, "bgr8")
            except CvBridgeError as e:
                print(e)

            try:
                # depth_image = self.bridge.imgmsg_to_cv2(depth_image_message, "32FC1")
                depth_image = self.bridge.imgmsg_to_cv2(self.depth_message, "32FC1")
            except CvBridgeError as e:
                print(e)
        else:
            return 0

        # Set the dimensions of the image
        self.dims = rgb_image.shape
        h = self.dims[0]
        w = self.dims[1]

        # Detect the faces in the image
        blob = cv2.dnn.blobFromImage(cv2.resize(rgb_image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
        self.face_net.setInput(blob)
        face_detections = self.face_net.forward()

        for i in range(0, face_detections.shape[2]):
            confidence = face_detections[0, 0, i, 2]
            if confidence>0.5:
                rospy.loginfo('Face detected')
                
                box = face_detections[0,0,i,3:7] * np.array([w,h,w,h])
                box = box.astype('int')
                x1, y1, x2, y2 = box[0], box[1], box[2], box[3]

                # Find the distance to the detected face
                face_distance = float(np.nanmean(depth_image[y1:y2,x1:x2]))
                if math.isnan(face_distance):
                    break
                
                rospy.loginfo('Distance to face %f', face_distance)

                # Get the time that the depth image was recieved
                depth_time = self.depth_message.header.stamp

                # Find the location of the detected face
                faceDetectionMsg = self.get_pose((x1,x2,y1,y2), face_distance, depth_time)

                # Determine if face has a mask
                face_region = rgb_image[y1:y2, x1:x2]
                wears_mask = self.find_mask(face_region);
                print("Wears mask?", wears_mask)

                faceDetectionMsg.wears_mask = wears_mask
                self.detection_publisher.publish(faceDetectionMsg)

def main():
    face_detector = face_detector_dnn()

    rate = rospy.Rate(1) # TODO: test
    while not rospy.is_shutdown():
        face_detector.find_faces()
        rate.sleep()

    cv2.destroyAllWindows()


if __name__ == '__main__':
    main()